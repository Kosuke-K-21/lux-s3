{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMP = \"lux-ai-season-3\"\n",
    "MAX_CALLS_PER_DAY = 3600  # Kaggle says don't do more than 3600 per day and 1 per second\n",
    "LOWEST_SCORE_THRESH = 1400\n",
    "\n",
    "META_DIR = \"../data/meta/\"\n",
    "REPLAY_DIR = \"../data/match/replay/\"\n",
    "INFO_DIR = \"../data/match/info/\"\n",
    "for d in [META_DIR, REPLAY_DIR, INFO_DIR]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "base_url = \"https://www.kaggle.com/api/i/competitions.EpisodeService/\"\n",
    "get_url = base_url + \"GetEpisodeReplay\"\n",
    "\n",
    "TIME_BUFFER = 1  # seconds\n",
    "COMPS = {\n",
    "    \"lux-ai-season-3\": 86411,\n",
    "    \"lux-ai-2022\": 45040,\n",
    "    \"kore-2022\": 34419,\n",
    "    \"lux-ai-2021\": 30067,\n",
    "    \"hungry-geese\": 25401,\n",
    "    \"rock-paper-scissors\": 22838,\n",
    "    \"santa-2020\": 24539,\n",
    "    \"halite\": 18011,\n",
    "    \"google-football\": 21723,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_df = pl.read_csv(META_DIR + \"Episodes.csv\")\n",
    "episodes_df = episodes_df.filter(pl.col(\"CompetitionId\") == COMPS[COMP])\n",
    "print(f\"Episodes.csv: {len(episodes_df)} rows after filtering for {COMP}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epagents_df = pl.read_csv(META_DIR + \"EpisodeAgents.csv\", schema_overrides={\"Reward\": pl.Float32})\n",
    "unique_comp_episode_ids = pl.Series(episodes_df.select(pl.col(\"Id\").unique())).to_list()\n",
    "epagents_df = epagents_df.filter(pl.col(\"EpisodeId\").is_in(unique_comp_episode_ids))\n",
    "\n",
    "print(f\"EpisodeAgents.csv: {len(epagents_df)} rows after filtering for {COMP}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Idはゲームで対戦したエージェントのユニークなID\n",
    "- EpisodeIdはゲーム毎のID\n",
    "- SubmissionIdは提出されたエージェントのID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epagents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欠損チェック\n",
    "display(episodes_df.null_count())\n",
    "display(epagents_df.null_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_df = episodes_df.with_columns(\n",
    "    pl.col(\"CreateTime\").str.to_datetime(\"%m/%d/%Y %H:%M:%S\"),\n",
    "    pl.col(\"EndTime\").str.to_datetime(\"%m/%d/%Y %H:%M:%S\"),\n",
    ")\n",
    "\n",
    "epagents_df = epagents_df.with_columns(\n",
    "    pl.col(\"InitialConfidence\").replace(\"\", np.nan).cast(pl.Float32),\n",
    "    pl.col(\"InitialScore\").replace(\"\", np.nan).cast(pl.Float32),\n",
    "    pl.col(\"UpdatedConfidence\").replace(\"\", np.nan).cast(pl.Float32),\n",
    "    pl.col(\"UpdatedScore\").replace(\"\", np.nan).cast(pl.Float32),\n",
    ")\n",
    "\n",
    "epagents_df = epagents_df.fill_nan(0.0)\n",
    "epagents_df = epagents_df.sort(\"Id\", descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = (\n",
    "    epagents_df.sort(\"EpisodeId\", descending=True)\n",
    "    .group_by(\"SubmissionId\")\n",
    "    .agg([pl.col(\"EpisodeId\").first(), pl.col(\"UpdatedScore\").first()])\n",
    ")\n",
    "\n",
    "max_df = max_df.filter(pl.col(\"UpdatedScore\") > LOWEST_SCORE_THRESH)\n",
    "sub_to_score_dict = dict(zip(max_df[\"SubmissionId\"], max_df[\"UpdatedScore\"]))\n",
    "print(f\"{len(sub_to_score_dict)} submissions with score over {LOWEST_SCORE_THRESH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get episodes for these submissions\n",
    "candidate_episodes = set()\n",
    "for key, _ in sorted(sub_to_score_dict.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    episodes = sorted(epagents_df.filter(pl.col(\"SubmissionId\") == key)[\"EpisodeId\"], reverse=True)\n",
    "    candidate_episodes.update(episodes)\n",
    "\n",
    "print(f\"{len(candidate_episodes)} episodes for these {len(sub_to_score_dict)} submissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for root, dirs, files in os.walk(REPLAY_DIR):\n",
    "    all_files.extend(files)\n",
    "\n",
    "seen_episodes = {int(file.split(\".\")[0]) for file in all_files if \".json\" in file and file.split(\".\")[0].isdigit()}\n",
    "unseen_episodes = candidate_episodes - seen_episodes\n",
    "\n",
    "print(f\"{len(unseen_episodes)} episodes out of the {len(candidate_episodes)} candidate episodes not yet saved\")\n",
    "print(f\"Total episodes saved: {len(seen_episodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_save_replay(episode_id):\n",
    "    re = requests.post(get_url, json={\"episodeId\": int(episode_id)})\n",
    "    replay = re.json()\n",
    "\n",
    "    with open(REPLAY_DIR + f\"{episode_id}.json\", \"w\") as f:\n",
    "        json.dump(replay, f)\n",
    "\n",
    "\n",
    "def save_replay_info(temp_episodes_df, temp_epagents_df):\n",
    "    create_seconds = int(temp_episodes_df[\"CreateTime\"].cast(pl.Float32).item() / 1e9)\n",
    "    end_seconds = int(temp_episodes_df[\"EndTime\"].cast(pl.Float32).item() / 1e9)\n",
    "\n",
    "    agents = []\n",
    "    for row in temp_epagents_df.rows(named=True):\n",
    "        agent = {\n",
    "            \"id\": int(row[\"Id\"]),\n",
    "            \"state\": int(row[\"State\"]),\n",
    "            \"submissionId\": int(row[\"SubmissionId\"]),\n",
    "            \"reward\": float(row[\"Reward\"]),\n",
    "            \"index\": int(row[\"Index\"]),\n",
    "            \"initialScore\": float(row[\"InitialScore\"]),\n",
    "            \"initialConfidence\": float(row[\"InitialConfidence\"]),\n",
    "            \"updatedScore\": float(row[\"UpdatedScore\"]),\n",
    "            \"updatedConfidence\": float(row[\"UpdatedConfidence\"]),\n",
    "        }\n",
    "\n",
    "        agents.append(agent)\n",
    "\n",
    "    info = {\n",
    "        \"id\": int(episode_id),\n",
    "        \"competitionId\": int(COMPS[COMP]),\n",
    "        \"createTime\": create_seconds,\n",
    "        \"endTime\": end_seconds,\n",
    "        \"agents\": agents,\n",
    "    }\n",
    "\n",
    "    with open(INFO_DIR + f\"{episode_id}.json\", \"w\") as f:\n",
    "        json.dump(info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "num_episodes_saved = 0\n",
    "\n",
    "num_api_calls_today = 0\n",
    "for key, _ in sorted(sub_to_score_dict.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    episodes_for_sub = sorted(epagents_df.filter(pl.col(\"SubmissionId\") == key)[\"EpisodeId\"], reverse=True)\n",
    "\n",
    "    for episode_id in episodes_for_sub:\n",
    "        if episode_id not in seen_episodes:\n",
    "            temp_episodes_df = episodes_df.filter(pl.col(\"Id\") == episode_id)\n",
    "            temp_epagents_df = epagents_df.filter(pl.col(\"EpisodeId\") == episode_id).sort(\"Index\", descending=True)\n",
    "\n",
    "            get_and_save_replay(episode_id)\n",
    "            save_replay_info(temp_episodes_df, temp_epagents_df)\n",
    "            num_episodes_saved += 1\n",
    "\n",
    "            if os.path.exists(REPLAY_DIR + f\"{episode_id}.json\") and os.path.exists(INFO_DIR + f\"{episode_id}.json\"):\n",
    "                print(str(num_api_calls_today) + f\": saved episode #{episode_id}\")\n",
    "                seen_episodes.add(episode_id)\n",
    "                num_api_calls_today += 1\n",
    "\n",
    "            else:\n",
    "                raise Exception(f\"Episode {episode_id} not saved\")\n",
    "\n",
    "            if time.time() - start_time < TIME_BUFFER:\n",
    "                time.sleep(TIME_BUFFER - (time.time() - start_time))\n",
    "\n",
    "        if num_api_calls_today > (min(3600, MAX_CALLS_PER_DAY)):\n",
    "            print(\"API call limit reached\")\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Episodes saved: {num_episodes_saved}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
